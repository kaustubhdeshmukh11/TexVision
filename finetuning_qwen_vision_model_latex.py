# -*- coding: utf-8 -*-
"""finetuning_qwen_vision_model_latex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MY62xty6aqc9N-B-8Fj0gjlWu_tSaYBL

---




1. like genraly we use quantized models from unsloth as thes are low memory footprint models there are many types like fp 32, fp 16 i.e ful precision 16 here the weights are in 16 bits aand so on


2.then in finetuning there are method like  lora and qlora
lora is low rank adaption where a small set of parameters of a model are updated and not all the parametrs are updated  **

3.then in fine tuning there is also sft i.e suppervised fine tunning where we need to give dataset in form of instruction and response too

4.In llm finetunning you genrally convert dataset into the chat template of that particularmodel

 like we have user: along with its instrution and data ...and then a agent(here our model) with its reponse for the instruction and data for a particular sample from the dataset this way entire dataset needs to be converted into the chat templte there are functions ..

 5. also now we have a trainer function which has various traing arguments in the trainer function


 6. if you want to finetune something you can go to datalake tutorial and search for finetuning there you will variousstep by step documentation of codes
"""

!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
!pip install --no-deps unsloth

from unsloth import FastVisionModel
import torch

fourbit_models = [
    "unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit",
    "unsloth/Qwen2-VL-7B-Instruct-bnb-4bit"
]

model, tokenizer = FastVisionModel.from_pretrained(
    "unsloth/Qwen2-VL-7B-Instruct",
    load_in_4bit=True,
    use_gradient_checkpointing="unsloth"
)

model = FastVisionModel.get_peft_model(
    model,
    finetune_vision_layers=True,
    finetune_language_layers=True,
    finetune_attention_modules=True,
    finetune_mlp_modules=True,

    r=16,
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    random_state = 3407,
    use_rslora=False,
    loftq_config=None
)

from datasets import load_dataset
dataset = load_dataset("unsloth/Latex_OCR", split="train")

dataset

dataset[0]

dataset[0]["image"]

dataset[1]["image"]

dataset[1]["text"]

instruction = "Write the LaTex representation for this image."

def convert_to_conversation(sample):
  conversation = [
      {"role": "user",
       "content": [
           {"type": "text", "text": instruction},
           {"type": "image", "image": sample["image"]}
       ]
       },
      {"role": "assistant",
       "content": [
           {"type": "text", "text": sample["text"]}
       ]
       }
  ]
  return {"messages": conversation}

convert_to_conversation(dataset[0])

converted_dataset = [convert_to_conversation(sample) for sample in dataset]

converted_dataset[1]

FastVisionModel.for_inference(model)

image = dataset[1]["image"]
messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": instruction},
            {"type": "image", "image": image}
        ]
    }
]

input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
inputs = tokenizer(
    image, input_text,
    add_special_tokens = False,
    return_tensors = "pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt=True)
_ = model.generate(**inputs, streamer= text_streamer, max_new_tokens = 128, use_cache=True, temperature=1.5, min_p=0.1)

image

from unsloth import is_bf16_supported
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig

FastVisionModel.for_inference(model)

trainer = SFTTrainer(
    model = model,
    tokenizer=tokenizer,
    data_collator = UnslothVisionDataCollator(model, tokenizer),
    train_dataset = converted_dataset,
    args = SFTConfig(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps = 5,
        max_steps=30,
        learning_rate=2e-4,
        fp16=not is_bf16_supported(),
        bf16 = is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
        report_to = "none",
        remove_unused_columns=False,
        dataset_text_field="",
        dataset_kwargs = {"skip_prepare_dataset": True},
        dataset_num_proc=4,
        max_seq_length=2048,
    ),
)

trainer.train()

FastVisionModel.for_inference(model)

image = dataset[2]["image"]

instruction = "Write the LaTeX representation for this image."

messages = [
    {"role": "user", "content": [
        {"type": "image"},
        {"type": "text", "text": instruction}
    ]}
]

input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)
inputs = tokenizer(
    image,
    input_text,
    add_special_tokens=False,
    return_tensors="pt",
).to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer, skip_prompt=True)

_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1)

image

